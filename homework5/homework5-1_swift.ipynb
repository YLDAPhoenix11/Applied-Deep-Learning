{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a5-1 swift.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD1BmBrddO6m",
        "colab_type": "text"
      },
      "source": [
        "# Swift for Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov_OVQlGeICK",
        "colab_type": "text"
      },
      "source": [
        "helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2FtcXhTcloX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Foundation\n",
        "\n",
        "var stderr = FileHandle.standardError\n",
        "\n",
        "extension FileHandle: TextOutputStream {\n",
        "    public func write(_ string: String) {\n",
        "        guard let data = string.data(using: .utf8) else { return }\n",
        "        self.write(data)\n",
        "    }\n",
        "}\n",
        "\n",
        "public func printError(_ message: String) {\n",
        "    print(message, to: &stderr)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tktiWpdmdQe0",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbCX0409dcI2",
        "colab_type": "text"
      },
      "source": [
        "LabeledExample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85x-L1dkbx-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import TensorFlow\n",
        "\n",
        "public struct LabeledExample: TensorGroup {\n",
        "    public var label: Tensor<Int32>\n",
        "    public var data: Tensor<Float>\n",
        "\n",
        "    public init(label: Tensor<Int32>, data: Tensor<Float>) {\n",
        "        self.label = label\n",
        "        self.data = data\n",
        "    }\n",
        "\n",
        "    public init<C: RandomAccessCollection>(\n",
        "        _handles: C\n",
        "    ) where C.Element: _AnyTensorHandle {\n",
        "        precondition(_handles.count == 2)\n",
        "        let labelIndex = _handles.startIndex\n",
        "        let dataIndex = _handles.index(labelIndex, offsetBy: 1)\n",
        "        label = Tensor<Int32>(handle: TensorHandle<Int32>(handle: _handles[labelIndex]))\n",
        "        data = Tensor<Float>(handle: TensorHandle<Float>(handle: _handles[dataIndex]))\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ULGh9zdduO",
        "colab_type": "text"
      },
      "source": [
        "ImageClassificationDataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZxCOvsbzaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public protocol ImageClassificationDataset {\n",
        "    init()\n",
        "    var trainingDataset: Dataset<LabeledExample> { get }\n",
        "    var testDataset: Dataset<LabeledExample> { get }\n",
        "    var trainingExampleCount: Int { get }\n",
        "    var testExampleCount: Int { get }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX9svRsbdkmd",
        "colab_type": "text"
      },
      "source": [
        "CIFAR10:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umcUUV9HXW_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Foundation\n",
        "import ModelSupport\n",
        "\n",
        "#if canImport(FoundationNetworking)\n",
        "    import FoundationNetworking\n",
        "#endif\n",
        "\n",
        "public struct CIFAR10: ImageClassificationDataset {\n",
        "    public let trainingDataset: Dataset<LabeledExample>\n",
        "    public let testDataset: Dataset<LabeledExample>\n",
        "    public let trainingExampleCount = 50000\n",
        "    public let testExampleCount = 10000\n",
        "\n",
        "    public init() {\n",
        "        self.init(\n",
        "            localStorageDirectory: FileManager.default.temporaryDirectory.appendingPathComponent(\n",
        "                \"CIFAR10\"))\n",
        "    }\n",
        "\n",
        "    public init(localStorageDirectory: URL) {\n",
        "        self.trainingDataset = Dataset<LabeledExample>(\n",
        "            elements: loadCIFARTrainingFiles(localStorageDirectory: localStorageDirectory))\n",
        "        self.testDataset = Dataset<LabeledExample>(\n",
        "            elements: loadCIFARTestFile(localStorageDirectory: localStorageDirectory))\n",
        "    }\n",
        "}\n",
        "\n",
        "func downloadCIFAR10IfNotPresent(to directory: URL) {\n",
        "    if !FileManager.default.fileExists(atPath: directory.path) {\n",
        "        do {\n",
        "            try FileManager.default.createDirectory(\n",
        "                at: directory, withIntermediateDirectories: false)\n",
        "        } catch {\n",
        "            fatalError(\n",
        "                \"Failed to create storage directory: \\(directory.path), error: \\(error)\"\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "\n",
        "    let downloadPath = directory.appendingPathComponent(\"cifar-10-batches-bin\").path\n",
        "    let directoryExists = FileManager.default.fileExists(atPath: downloadPath)\n",
        "\n",
        "    guard !directoryExists else { return }\n",
        "\n",
        "    printError(\"Downloading CIFAR dataset...\")\n",
        "    let archivePath = directory.appendingPathComponent(\"cifar-10-binary.tar.gz\").path\n",
        "    let archiveExists = FileManager.default.fileExists(atPath: archivePath)\n",
        "    if !archiveExists {\n",
        "        printError(\"Archive missing, downloading...\")\n",
        "        do {\n",
        "            let downloadedFile = try Data(\n",
        "                contentsOf: URL(\n",
        "                    string: \"https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\")!)\n",
        "            try downloadedFile.write(to: URL(fileURLWithPath: archivePath))\n",
        "        } catch {\n",
        "            printError(\"Could not download CIFAR dataset, error: \\(error)\")\n",
        "            exit(-1)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printError(\"Archive downloaded, processing...\")\n",
        "\n",
        "    #if os(macOS)\n",
        "        let tarLocation = \"/usr/bin/tar\"\n",
        "    #else\n",
        "        let tarLocation = \"/bin/tar\"\n",
        "    #endif\n",
        "\n",
        "    let task = Process()\n",
        "    task.executableURL = URL(fileURLWithPath: tarLocation)\n",
        "    task.arguments = [\"xzf\", archivePath, \"-C\", directory.path]\n",
        "    do {\n",
        "        try task.run()\n",
        "        task.waitUntilExit()\n",
        "    } catch {\n",
        "        printError(\"CIFAR extraction failed with error: \\(error)\")\n",
        "    }\n",
        "\n",
        "    do {\n",
        "        try FileManager.default.removeItem(atPath: archivePath)\n",
        "    } catch {\n",
        "        printError(\"Could not remove archive, error: \\(error)\")\n",
        "        exit(-1)\n",
        "    }\n",
        "\n",
        "    printError(\"Unarchiving completed\")\n",
        "}\n",
        "\n",
        "func loadCIFARFile(named name: String, in directory: URL) -> LabeledExample {\n",
        "    downloadCIFAR10IfNotPresent(to: directory)\n",
        "    let path = directory.appendingPathComponent(\"cifar-10-batches-bin/\\(name)\").path\n",
        "\n",
        "    let imageCount = 10000\n",
        "    guard let fileContents = try? Data(contentsOf: URL(fileURLWithPath: path)) else {\n",
        "        printError(\"Could not read dataset file: \\(name)\")\n",
        "        exit(-1)\n",
        "    }\n",
        "    guard fileContents.count == 30_730_000 else {\n",
        "        printError(\n",
        "            \"Dataset file \\(name) should have 30730000 bytes, instead had \\(fileContents.count)\")\n",
        "        exit(-1)\n",
        "    }\n",
        "\n",
        "    var bytes: [UInt8] = []\n",
        "    var labels: [Int64] = []\n",
        "\n",
        "    let imageByteSize = 3073\n",
        "    for imageIndex in 0..<imageCount {\n",
        "        let baseAddress = imageIndex * imageByteSize\n",
        "        labels.append(Int64(fileContents[baseAddress]))\n",
        "        bytes.append(contentsOf: fileContents[(baseAddress + 1)..<(baseAddress + 3073)])\n",
        "    }\n",
        "\n",
        "    let labelTensor = Tensor<Int64>(shape: [imageCount], scalars: labels)\n",
        "    let images = Tensor<UInt8>(shape: [imageCount, 3, 32, 32], scalars: bytes)\n",
        "\n",
        "    // Transpose from the CIFAR-provided N(CHW) to TF's default NHWC.\n",
        "    let imageTensor = Tensor<Float>(images.transposed(permutation: [0, 2, 3, 1]))\n",
        "\n",
        "    let mean = Tensor<Float>([0.485, 0.456, 0.406])\n",
        "    let std = Tensor<Float>([0.229, 0.224, 0.225])\n",
        "    let imagesNormalized = ((imageTensor / 255.0) - mean) / std\n",
        "\n",
        "    return LabeledExample(label: Tensor<Int32>(labelTensor), data: imagesNormalized)\n",
        "}\n",
        "\n",
        "func loadCIFARTrainingFiles(localStorageDirectory: URL) -> LabeledExample {\n",
        "    let data = (1..<6).map {\n",
        "        loadCIFARFile(named: \"data_batch_\\($0).bin\", in: localStorageDirectory)\n",
        "    }\n",
        "    return LabeledExample(\n",
        "        label: Tensor(concatenating: data.map { $0.label }, alongAxis: 0),\n",
        "        data: Tensor(concatenating: data.map { $0.data }, alongAxis: 0)\n",
        "    )\n",
        "}\n",
        "\n",
        "func loadCIFARTestFile(localStorageDirectory: URL) -> LabeledExample {\n",
        "    return loadCIFARFile(named: \"test_batch.bin\", in: localStorageDirectory)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6xeTTH8eA60",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM4ui-tUXW81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// Ported from github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
        "struct KerasModel: Layer {\n",
        "    typealias Input = Tensor<Float>\n",
        "    typealias Output = Tensor<Float>\n",
        "\n",
        "    var conv1a = Conv2D<Float>(filterShape: (3, 3, 3, 32), padding: .same, activation: relu)\n",
        "    var conv1b = Conv2D<Float>(filterShape: (3, 3, 32, 32), activation: relu)\n",
        "    var pool1 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))\n",
        "    var dropout1 = Dropout<Float>(probability: 0.25)\n",
        "    var conv2a = Conv2D<Float>(filterShape: (3, 3, 32, 64), padding: .same, activation: relu)\n",
        "    var conv2b = Conv2D<Float>(filterShape: (3, 3, 64, 64), activation: relu)\n",
        "    var pool2 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))\n",
        "    var dropout2 = Dropout<Float>(probability: 0.25)\n",
        "    var flatten = Flatten<Float>()\n",
        "    var dense1 = Dense<Float>(inputSize: 64 * 6 * 6, outputSize: 512, activation: relu)\n",
        "    var dropout3 = Dropout<Float>(probability: 0.5)\n",
        "    var dense2 = Dense<Float>(inputSize: 512, outputSize: 10, activation: identity)\n",
        "\n",
        "    @differentiable\n",
        "    func callAsFunction(_ input: Input) -> Output {\n",
        "        let conv1 = input.sequenced(through: conv1a, conv1b, pool1, dropout1)\n",
        "        let conv2 = conv1.sequenced(through: conv2a, conv2b, pool2, dropout2)\n",
        "        return conv2.sequenced(through: flatten, dense1, dropout3, dense2)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyqHlPhoefWl",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGo7ChlMXX1J",
        "colab_type": "code",
        "outputId": "6b572710-c7bf-4af7-c864-aa6a5c834954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "let batchSize = 32\n",
        "\n",
        "let dataset = CIFAR10()\n",
        "let testBatches = dataset.testDataset.batched(batchSize)\n",
        "\n",
        "var model = KerasModel()\n",
        "let optimizer = RMSProp(for: model, learningRate: 0.0001, decay: 1e-6)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in 1...20 {\n",
        "    Context.local.learningPhase = .training\n",
        "    var trainingLossSum: Float = 0\n",
        "    var trainingBatchCount = 0\n",
        "    let trainingShuffled = dataset.trainingDataset.shuffled(\n",
        "        sampleCount: dataset.trainingExampleCount, randomSeed: Int64(epoch))\n",
        "    for batch in trainingShuffled.batched(batchSize) {\n",
        "        let (labels, images) = (batch.label, batch.data)\n",
        "        let (loss, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in\n",
        "            let logits = model(images)\n",
        "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
        "        }\n",
        "        trainingLossSum += loss.scalarized()\n",
        "        trainingBatchCount += 1\n",
        "        optimizer.update(&model, along: gradients)\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    var testLossSum: Float = 0\n",
        "    var testBatchCount = 0\n",
        "    var correctGuessCount = 0\n",
        "    var totalGuessCount = 0\n",
        "    for batch in testBatches {\n",
        "        let (labels, images) = (batch.label, batch.data)\n",
        "        let logits = model(images)\n",
        "        testLossSum += softmaxCrossEntropy(logits: logits, labels: labels).scalarized()\n",
        "        testBatchCount += 1\n",
        "\n",
        "        let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels\n",
        "        correctGuessCount = correctGuessCount\n",
        "            + Int(\n",
        "                Tensor<Int32>(correctPredictions).sum().scalarized())\n",
        "        totalGuessCount = totalGuessCount + batchSize\n",
        "    }\n",
        "\n",
        "    let accuracy = Float(correctGuessCount) / Float(totalGuessCount)\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Accuracy: \\(correctGuessCount)/\\(totalGuessCount) (\\(accuracy)) \\\n",
        "        Loss: \\(testLossSum / Float(testBatchCount))\n",
        "        \"\"\"\n",
        "    )\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "[Epoch 1] Accuracy: 4989/10016 (0.49810302) Loss: 1.431187\n",
            "[Epoch 2] Accuracy: 5582/10016 (0.5573083) Loss: 1.2572505\n",
            "[Epoch 3] Accuracy: 6011/10016 (0.6001398) Loss: 1.1344422\n",
            "[Epoch 4] Accuracy: 6396/10016 (0.6385783) Loss: 1.0295383\n",
            "[Epoch 5] Accuracy: 6591/10016 (0.65804714) Loss: 0.9724738\n",
            "[Epoch 6] Accuracy: 6870/10016 (0.68590254) Loss: 0.9059218\n",
            "[Epoch 7] Accuracy: 6867/10016 (0.685603) Loss: 0.88318604\n",
            "[Epoch 8] Accuracy: 7128/10016 (0.71166134) Loss: 0.8305917\n",
            "[Epoch 9] Accuracy: 7201/10016 (0.7189497) Loss: 0.8110487\n",
            "[Epoch 10] Accuracy: 7332/10016 (0.7320288) Loss: 0.77304375\n",
            "[Epoch 11] Accuracy: 7288/10016 (0.7276358) Loss: 0.7856382\n",
            "[Epoch 12] Accuracy: 7412/10016 (0.740016) Loss: 0.7428276\n",
            "[Epoch 13] Accuracy: 7417/10016 (0.7405152) Loss: 0.75255996\n",
            "[Epoch 14] Accuracy: 7416/10016 (0.74041533) Loss: 0.7470886\n",
            "[Epoch 15] Accuracy: 7504/10016 (0.7492013) Loss: 0.7293965\n",
            "[Epoch 16] Accuracy: 7426/10016 (0.7414137) Loss: 0.7562756\n",
            "[Epoch 17] Accuracy: 7621/10016 (0.7608826) Loss: 0.7068832\n",
            "[Epoch 18] Accuracy: 7564/10016 (0.7551917) Loss: 0.7175982\n",
            "[Epoch 19] Accuracy: 7666/10016 (0.7653754) Loss: 0.6899548\n",
            "[Epoch 20] Accuracy: 7632/10016 (0.76198083) Loss: 0.6888127\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}